mm/mempool.c

void * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)
{
 void *element;
 unsigned long flags;
 wait_queue_t wait;
 gfp_t gfp_temp;


 might_sleep_if(gfp_mask & __GFP_WAIT);


 gfp_mask |= __GFP_NOMEMALLOC; /* don't allocate emergency reserves */
 gfp_mask |= __GFP_NORETRY; /* don't loop in __alloc_pages */
 gfp_mask |= __GFP_NOWARN; /* failures are OK */


 gfp_temp = gfp_mask & ~(__GFP_WAIT|__GFP_IO);


repeat_alloc:


 element = pool->alloc(gfp_temp, pool->pool_data);
 if (likely(element != NULL))
 return element;


 spin_lock_irqsave(&pool->lock, flags);
 if (likely(pool->curr_nr)) {
 element = remove_element(pool);
 spin_unlock_irqrestore(&pool->lock, flags);
 /* paired with rmb in mempool_free(), read comment there */
 smp_wmb();
 return element;
 }


 /*
  * We use gfp mask w/o __GFP_WAIT or IO for the first round.  If
  * alloc failed with that and @pool was empty, retry immediately.
  */
 if (gfp_temp != gfp_mask) {
 spin_unlock_irqrestore(&pool->lock, flags);
 gfp_temp = gfp_mask;
 goto repeat_alloc;
 }


 /* We must not sleep if !__GFP_WAIT */
 if (!(gfp_mask & __GFP_WAIT)) {
 spin_unlock_irqrestore(&pool->lock, flags);
 return NULL;
 }


 /* Let's wait for someone else to return an element to @pool */
 init_wait(&wait);
 prepare_to_wait(&pool->wait, &wait, TASK_UNINTERRUPTIBLE);
    //wjx:由于这里是把进程状态设置为TASK_UNINTERRUPTIBLE后才释放锁，而且只有在
    //mempool_free()获得锁并把element还回pool后才会唤醒这个队列，所以这里可以确保在prepare_to_wait()
    //之前并没有人调用mempool_free()把element归还pool里（因为持有锁），也就是说这里不会无谓的等待。
    //而prepare_to_wait()之后被唤醒则没有关系，这样下面的schedule()不会睡眠。
 spin_unlock_irqrestore(&pool->lock, flags);


 /*
  * FIXME: this should be io_schedule().  The timeout is there as a
  * workaround for some DM problems in 2.6.18.
  */
 io_schedule_timeout(5*HZ);


 finish_wait(&pool->wait, &wait);
 goto repeat_alloc;
}
EXPORT_SYMBOL(mempool_alloc);

void mempool_free(void *element, mempool_t *pool)
{
 unsigned long flags;


 if (unlikely(element == NULL))
 return;


 /*
  * Paired with the wmb in mempool_alloc().  The preceding read is
  * for @element and the following @pool->curr_nr.  This ensures
  * that the visible value of @pool->curr_nr is from after the
  * allocation of @element.  This is necessary for fringe cases
  * where @element was passed to this task without going through
  * barriers.
  *
  * For example, assume @p is %NULL at the beginning and one task
  * performs "p = mempool_alloc(...);" while another task is doing
  * "while (!p) cpu_relax(); mempool_free(p, ...);".  This function
  * may end up using curr_nr value which is from before allocation
  * of @p without the following rmb.
  */
 smp_rmb();


 /*
  * For correctness, we need a test which is guaranteed to trigger
  * if curr_nr + #allocated == min_nr.  Testing curr_nr < min_nr
  * without locking achieves that and refilling as soon as possible
  * is desirable.
  *
  * Because curr_nr visible here is always a value after the
  * allocation of @element, any task which decremented curr_nr below
  * min_nr is guaranteed to see curr_nr < min_nr unless curr_nr gets
  * incremented to min_nr afterwards.  If curr_nr gets incremented
  * to min_nr after the allocation of @element, the elements
  * allocated after that are subject to the same guarantee.
  *
  * Waiters happen iff curr_nr is 0 and the above guarantee also
  * ensures that there will be frees which return elements to the
  * pool waking up the waiters.
  */
 if (pool->curr_nr < pool->min_nr) {
 spin_lock_irqsave(&pool->lock, flags);
 if (pool->curr_nr < pool->min_nr) {
 add_element(pool, element);
 spin_unlock_irqrestore(&pool->lock, flags);
            //wjx:有element被归还到pool后才唤醒wait队列。
 wake_up(&pool->wait);
 return;
 }
 spin_unlock_irqrestore(&pool->lock, flags);
 }
 pool->free(element, pool->pool_data);
}
EXPORT_SYMBOL(mempool_free);
